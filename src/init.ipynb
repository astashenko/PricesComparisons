{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymssql\n",
    "import cx_Oracle\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import ExcelWriter\n",
    "import itertools\n",
    "from pandas.testing import assert_frame_equal\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "# plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image # converting images into arrays\n",
    "\n",
    "pio.renderers.default = 'iframe+vscode'  # не знаю почему в jupyter без него не показывались графики\n",
    "\n",
    "pd.set_option('display.max_columns', 60) # set so can see all columns of the DataFrame\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score,  recall_score, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.feature_selection import f_classif, chi2, f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.utils import class_weight as sk_class_weight\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats.distributions as distributions\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "import os, shutil,  os.path, tarfile\n",
    "import networkx as nx\n",
    "\n",
    "from pandas.core.dtypes.common import is_timedelta64_dtype\n",
    "\n",
    "from abc import ABC, abstractmethod, abstractproperty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_side_by_side вывод несколько таблиц\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def html_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display: inline-block\"')\n",
    "        html_str+='</td></th>'\n",
    "    return html_str\n",
    "\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        html_str+=f'<h2>{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display: inline-block\"')\n",
    "        html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prError(s): print(\"\\033[91m {}\\033[00m\" .format(s))\n",
    "\n",
    "def Check_length(len1, len2):\n",
    "    \n",
    "    # assert len1 == len2, 'Длина различна {} versus {}'.format(len1, len2)\n",
    "    \n",
    "    if (len1 != len2):\n",
    "        prError('Длина различна {}  -  {}'.format(len1, len2))\n",
    "        \n",
    "def Check_dupl(n, duplicates):\n",
    "    if (n != 0):\n",
    "        prError('Ошибка. Количество дубликатов {}'.format(n))  \n",
    "        display_side_by_side(duplicates.head())  \n",
    "\n",
    "def identify_duplicates(df, columns):\n",
    "    \"\"\"identifies duplicates in columns of DataFrame \"\"\"\n",
    " \n",
    "    rows = df.groupby(columns).size().reset_index()\n",
    "    rows.rename({0: 'row_nums'}, axis = 1, inplace = True)\n",
    "\n",
    "    dupl = rows[rows['row_nums'] > 1] # дубликаты\n",
    "    \n",
    "    return len(dupl), pd.merge(df, dupl, how = 'inner', on = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_plotly_figs_to_html(plotly_figs, html_fname, include_plotlyjs='cdn', \n",
    "                                separator=None, auto_open=False):\n",
    "    with open(html_fname, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(plotly_figs[0].to_html(include_plotlyjs=include_plotlyjs))\n",
    "        for fig in plotly_figs[1:]:\n",
    "            if separator:\n",
    "                f.write(separator)\n",
    "            if isinstance(fig, str):\n",
    "                f.write(fig)\n",
    "            else:\n",
    "                f.write(fig.to_html(full_html=False, include_plotlyjs=False))\n",
    "\n",
    "    if auto_open:\n",
    "        import pathlib, webbrowser\n",
    "        uri = pathlib.Path(html_fname).absolute().as_uri()\n",
    "        webbrowser.open(uri)\n",
    "# combine_plotly_figs_to_html([fig1, fig2], 'p_graph2.html', separator = 'Second option', auto_open=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Очистить содержимое папки, используется для iframe_figures\"\"\"\n",
    "def clear_folder(folder):\n",
    "    if not os.path.isdir(folder):\n",
    "        return\n",
    "    for filename in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "\"\"\" Пример\n",
    "name = pre+\"data/external/20230131_portfolio.pkl\"\n",
    "targz_name = pre+\"data/external/20230131_portfolio.tar.gz\"\n",
    "make_tarfile(targz_name, name)\n",
    "\"\"\"\n",
    "\n",
    "def extract_all(archive, extract_path):\n",
    "    if archive.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(archive, \"r:gz\")\n",
    "        tar.extractall(extract_path)\n",
    "        tar.close()\n",
    "    elif archive.endswith(\"tar\"):\n",
    "        tar = tarfile.open(archive, \"r:\")\n",
    "        tar.extractall(extract_path)\n",
    "        tar.close()\n",
    "\n",
    "\"\"\" Пример \n",
    "name = pre + 'data/external/20230131_portfolio.tar.gz'\n",
    "extract_all(name, pre +'data/interm/external_pkl')   \n",
    "\"\"\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_fields(df, etalon, column_to_check):\n",
    "    df_filled = df[df[etalon].notnull()]\n",
    "    mask = (df_filled[etalon]!=df_filled[column_to_check])\n",
    "\n",
    "    tmp = df_filled[mask]\n",
    "\n",
    "    if (len(tmp)>0):\n",
    "        print(len(tmp))\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ComparisonDataFrames():\n",
    "    \n",
    "    def __init__(self, data_1, data_2, **kwargs):\n",
    "        \n",
    "        actual = kwargs.get('actual', None)\n",
    "        sourcesName = kwargs.get('sourcesName', ['df_1', 'df_2'])\n",
    "        suffix = kwargs.get('suffix', '_df2')\n",
    "        on = kwargs.get('on', None)\n",
    "        left_on = kwargs.get('left_on', None)\n",
    "        right_on = kwargs.get('right_on', None) \n",
    "        base_col = kwargs.get('base_col', None) \n",
    "        notnull_col = kwargs.get('notnull_col', None) \n",
    "        \n",
    "        if on is not None:\n",
    "            left_on = on\n",
    "            right_on = on\n",
    "        \n",
    "        notes = pd.DataFrame(data = data_1.columns, columns=['Поле'])\n",
    "        notes['Источник'] = sourcesName[0]\n",
    "        if actual:\n",
    "            notes['Дата актуальности'] = actual[0]\n",
    "\n",
    "        tmp = pd.DataFrame(data = data_2.columns, columns=['Поле'])\n",
    "        tmp['Источник'] = sourcesName[1]\n",
    "        if actual:\n",
    "            tmp['Дата актуальности'] = actual[1]\n",
    "        notes = notes.append(tmp, ignore_index=True)\n",
    "        \n",
    "        self.data_1 = data_1\n",
    "        self.data_2 = data_2\n",
    "        self.notes = notes\n",
    "        self.left_on = left_on\n",
    "        self.right_on = right_on\n",
    "        self.suffix = suffix\n",
    "        self.sourcesName = sourcesName\n",
    "        self.full_merge = None\n",
    "        self.inner_merge = None\n",
    "        self.base_col = base_col\n",
    "        self.notnull_col = notnull_col\n",
    "        \n",
    "        error = self.check_duplicates()\n",
    "        \n",
    "        if not error:\n",
    "            self.full_merge = self.merge()\n",
    "            self.inner_merge = self.inner()\n",
    "\n",
    "    def check_duplicates(self):\n",
    "        \n",
    "        error = False\n",
    "        num, duplicates = identify_duplicates(self.data_1, self.left_on)\n",
    "        if (num != 0):\n",
    "            error = True\n",
    "            prError('Ошибка. Количество дубликатов {} в {}'.format(num, self.sourcesName[0]))  \n",
    "            display_side_by_side(duplicates.head())  \n",
    "        \n",
    "        num, duplicates = identify_duplicates(self.data_2,  self.right_on)\n",
    "        if (num != 0):\n",
    "            error = True\n",
    "            prError('Ошибка. Количество дубликатов {} в {}'.format(num, self.sourcesName[1]))  \n",
    "            display_side_by_side(duplicates.head())  \n",
    "\n",
    "        return error\n",
    "           \n",
    "    def merge(self):\n",
    "        \n",
    "        full_merge = pd.merge(self.data_1, self.data_2, how = 'outer', \n",
    "                              left_on=self.left_on, right_on=self.right_on, \n",
    "                              suffixes=['', self.suffix])\n",
    "        # определяем дубликаты по ID\n",
    "        num, duplicates = identify_duplicates(full_merge, self.left_on)\n",
    "        Check_dupl(num, duplicates)\n",
    "        \n",
    "        num, duplicates = identify_duplicates(full_merge, self.right_on)\n",
    "        Check_dupl(num, duplicates)\n",
    "\n",
    "        return full_merge\n",
    "    \n",
    "    def inner(self):\n",
    "        \n",
    "        inner_merge = pd.merge(self.data_1, self.data_2, how = 'inner', \n",
    "                              left_on=self.left_on, right_on=self.right_on, \n",
    "                              suffixes=['', self.suffix])\n",
    "        return inner_merge\n",
    " \n",
    "    def missedRows(self, num_df=0, sourcesName=None, **kwargs):\n",
    "        add_condition = kwargs.get('add_condition', True)\n",
    "        \n",
    "        if sourcesName:\n",
    "            num_df = self.sourcesName.index(sourcesName)\n",
    "        key_col = self.notnull_col[num_df]\n",
    "        \n",
    "        # проверяем, что к notnull не было suffix\n",
    "        if num_df==1 and (key_col + self.suffix in self.full_merge.columns):\n",
    "            key_col = key_col + self.suffix \n",
    "            print('Проверка по столбцу {}'.format(key_col))\n",
    "\n",
    "        # приложений, которых нет в dataframe\n",
    "        mask_miss = (self.full_merge[key_col].isnull())\n",
    "\n",
    "        missed = self.full_merge[mask_miss & (add_condition)]\n",
    "        print(\"отсутствуют в {} {} строк\".format(self.sourcesName[num_df], len(missed)))\n",
    "        \n",
    "        return missed\n",
    "    \n",
    "    def presentRows(self, **kwargs):\n",
    "                \n",
    "        # приложения, которые присутствуют в обоих dataframe\n",
    "        print(\"одновременно присутствуют в {} и {} {} строк\".format(self.sourcesName[0], self.sourcesName[1],  len(self.inner_merge)))\n",
    "        \n",
    "        return self.inner_merge\n",
    "    \n",
    "    def check_fields(self, column_1, column_2, **kwargs):\n",
    "        \n",
    "        add_condition = kwargs.get('add_condition', True)\n",
    "        restriction = \"add_condition\" in kwargs\n",
    "        \n",
    "        df = self.inner_merge\n",
    "        if column_2 + self.suffix in self.inner_merge.columns:\n",
    "            column_2 = column_2 + self.suffix \n",
    "            print('Проверка по столбцам {} и {}'.format(column_1, column_2))\n",
    "        \n",
    "        mask = (df[column_1]!=df[column_2])\n",
    "\n",
    "        notCoincide = df[mask & (add_condition)].copy()\n",
    "\n",
    "        \n",
    "        if (len(notCoincide)>0):\n",
    "            print('значения не совпадают у {} строк из {}'.format(len(notCoincide), len(df)))\n",
    "        else:\n",
    "            print('значения совпадают во всех {} строках'.format(len(df)))\n",
    "        \n",
    "        if restriction:\n",
    "           print('с учетом ограничения проверялось {} строк'.format(len(df[(add_condition)])))\n",
    "           \n",
    "        if self.base_col:\n",
    "            keep_col = self.base_col.copy()\n",
    "            keep_col.append(column_1)\n",
    "            keep_col.append(column_2)  \n",
    "              \n",
    "        return notCoincide[keep_col]  if self.base_col else notCoincide\n",
    "# kwargs = dict( actual = [portfolio['REPORT_DT'].min(), portfolio_tsal['REPORT_DT'].min()], \n",
    "#                sourcesName = ['portfolio', 'tsal_leas_portfolio'], \n",
    "#                suffix = '_tsal', on= ['CONTRACT_KEY']\n",
    "#              )\n",
    "\n",
    "# comp =  ComparisonDataFrames(portfolio, portfolio_tsal, **kwargs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import Formatter\n",
    "def strfdelta(tdelta, fmt='{D:02}d {H:02}h {M:02}m {S:02}s', inputtype='timedelta'):\n",
    "    \"\"\"Convert a datetime.timedelta object or a regular number to a custom-\n",
    "    formatted string, just like the stftime() method does for datetime.datetime\n",
    "    objects.\n",
    "\n",
    "    The fmt argument allows custom formatting to be specified.  Fields can \n",
    "    include seconds, minutes, hours, days, and weeks.  Each field is optional.\n",
    "\n",
    "    Some examples:\n",
    "        '{D:02}d {H:02}h {M:02}m {S:02}s' --> '05d 08h 04m 02s' (default)\n",
    "        '{W}w {D}d {H}:{M:02}:{S:02}'     --> '4w 5d 8:04:02'\n",
    "        '{D:2}d {H:2}:{M:02}:{S:02}'      --> ' 5d  8:04:02'\n",
    "        '{H}h {S}s'                       --> '72h 800s'\n",
    "\n",
    "    The inputtype argument allows tdelta to be a regular number instead of the  \n",
    "    default, which is a datetime.timedelta object.  Valid inputtype strings: \n",
    "        's', 'seconds', \n",
    "        'm', 'minutes', \n",
    "        'h', 'hours', \n",
    "        'd', 'days', \n",
    "        'w', 'weeks'\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert tdelta to integer seconds.\n",
    "    if inputtype == 'timedelta':\n",
    "        remainder = int(tdelta.total_seconds())\n",
    "    elif inputtype in ['s', 'seconds']:\n",
    "        remainder = int(tdelta)\n",
    "    elif inputtype in ['m', 'minutes']:\n",
    "        remainder = int(tdelta)*60\n",
    "    elif inputtype in ['h', 'hours']:\n",
    "        remainder = int(tdelta)*3600\n",
    "    elif inputtype in ['d', 'days']:\n",
    "        remainder = int(tdelta)*86400\n",
    "    elif inputtype in ['w', 'weeks']:\n",
    "        remainder = int(tdelta)*604800\n",
    "\n",
    "    f = Formatter()\n",
    "    desired_fields = [field_tuple[1] for field_tuple in f.parse(fmt)]\n",
    "    possible_fields = ('Y', 'W', 'D', 'H', 'M', 'S')\n",
    "    constants = {'Y': 31536000, 'W': 604800, 'D': 86400, 'H': 3600, 'M': 60, 'S': 1}\n",
    "    values = {}\n",
    "    for field in possible_fields:\n",
    "        if field in desired_fields and field in constants:\n",
    "            values[field], remainder = divmod(remainder, constants[field])\n",
    "    return f.format(fmt, **values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvl(var, val):\n",
    "  if var is None:\n",
    "    return val\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, column, values):\n",
    "    if isinstance(values, str):\n",
    "        values = [values]\n",
    "    return df[df[column].isin(values)]\n",
    "\n",
    "def get_data_by_inn(df, inns):\n",
    "    list_of_inn = ['new_lessee_inn', 'ИНН контрагента', 'NEW_INN', 'INN', 'ИНН']\n",
    "    \n",
    "    column_inn = list(set(list_of_inn) & set(df.columns))[0]\n",
    "    return get_data(df, column_inn, inns)\n",
    "\n",
    "def get_contract_by_deal(df, deals):\n",
    "    return get_data(df, 'Номер сделки', deals)\n",
    "\n",
    "def get_data_by_application(df, applications):\n",
    "    return get_data(df, 'Приложение', applications)\n",
    "\n",
    "def get_data_by_name(df, contract_names):\n",
    "    list_of_contract_name = ['Номер договора (кратко)', 'Номер договора', 'CONTRACT_NUM']\n",
    "    \n",
    "    column_contract_name = list(set(list_of_contract_name) & set(df.columns))[0]\n",
    "    return get_data(df, column_contract_name, contract_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_oracle(sql_query, title='', params=dict()):\n",
    "    \n",
    "    cnxn_oracle = cx_Oracle.connect(obi_login, obi_pw, \"10.0.2.45:1521/UAKRPDB\", encoding=\"UTF-8\") \n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(sql_query, cnxn_oracle, params=params)\n",
    "        print(\"Размер считанных данных {} - {}\".format(title, df.shape))\n",
    "\n",
    "    finally:\n",
    "        cnxn_oracle.close() \n",
    "        \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
